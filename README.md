# aiFeelNews ğŸ“°ğŸ’­

> **AI-powered news sentiment analysis platform for university project with research purposes**

A FastAPI-based news aggregation and sentiment analysis platform that ingests articles from Mediastack API, processes them with sentiment analysis, and provides a RESTful API for accessing news with emotional context.

## ğŸ¯ Project Overview

**Core Features:**
- ğŸ“° News ingestion from Mediastack API
- ğŸ•·ï¸ Ethical web crawling with robots.txt compliance
- ğŸ§  Sentiment analysis (VADER â†’ Google Cloud NL)
- ğŸ—„ï¸ PostgreSQL database with proper data minimization
- ğŸš€ FastAPI REST API with OpenAPI docs
- â° TTL-based content cleanup for privacy compliance
- ğŸ›¡ï¸ Cybersecurity-compliant crawling with rate limiting
- ğŸ“… Automated Cloud Scheduler jobs with API optimization

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- PostgreSQL (or use Docker Compose)
- Mediastack API key

### Local Development

```bash
# Clone and setup
git clone <repository-url>
cd aifeelnews

# Install dependencies
pip install -r requirements.txt

# Environment setup
cp .env.example .env
# Edit .env with your Mediastack API key and database URL

# Database setup
alembic upgrade head

# Run ingestion pipeline
python -m app.jobs.run_ingestion

# Run crawl worker separately
python app/jobs/run_crawl_worker.py

# Start API server
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Multi-Container Docker

```bash
# Start all services (web API, worker, scheduler, database)
docker-compose up -d

# Check service health
docker-compose ps
curl http://localhost:8080/health

# View logs for specific services
docker-compose logs web
docker-compose logs worker
docker-compose logs scheduler

# Run one-time ingestion
docker-compose exec scheduler python -m app.jobs.run_ingestion

# Stop all services
docker-compose down
```

### Production Docker

```bash
# Test production configuration locally
docker-compose -f docker-compose.prod.yml build
docker-compose -f docker-compose.prod.yml up

# Build individual services
docker build -f docker/Dockerfile.web -t aifeelnews-web .
docker build -f docker/Dockerfile.worker -t aifeelnews-worker .
docker build -f docker/Dockerfile.scheduler -t aifeelnews-scheduler .
```

## ğŸ“Š API Endpoints

| Endpoint | Method | Description |
|----------|---------|-------------|
| `/docs` | GET | Interactive API documentation |
| `/health` | GET | Service health check with database connectivity |
| `/ready` | GET | Kubernetes readiness check |
| `/articles` | GET | List articles with sentiment data |
| `/articles/{id}` | GET | Get specific article details |
| `/sources` | GET | List configured news sources |
| `/users` | GET/POST | User management |
| `/bookmarks` | GET/POST | User bookmarks |
| `/api/v1/trigger-ingestion` | POST | **Cloud Scheduler**: Trigger news ingestion pipeline |
| `/api/v1/cleanup` | POST | **Cloud Scheduler**: Database maintenance and TTL cleanup |

**Example API Response:**
```json
{
  "id": 1,
  "title": "Breaking: AI Revolution in Healthcare",
  "description": "New developments in medical AI...",
  "url": "https://example.com/article",
  "published_at": "2024-11-18T15:30:00Z",
  "sentiment_label": "positive",
  "sentiment_score": 0.8472,
  "source": {
    "name": "TechNews",
    "domain": "technews.com"
  }
}
```

## ğŸ—‚ï¸ Project Structure

**Multi-Container Architecture:** Separate services for scalability and deployment flexibility.

```
aifeelnews/
â”œâ”€â”€ docker/                     # Container configurations
â”‚   â”œâ”€â”€ Dockerfile.web          # FastAPI web service
â”‚   â”œâ”€â”€ Dockerfile.worker       # Background crawl worker
â”‚   â”œâ”€â”€ Dockerfile.scheduler    # Scheduled ingestion jobs
â”‚   â””â”€â”€ README.md              # Docker documentation
â”œâ”€â”€ app/                       # Application code
â”‚   â”œâ”€â”€ config/                # Organized configuration classes
â”‚   â”‚   â”œâ”€â”€ __init__.py        # Main config object & legacy compatibility
â”‚   â”‚   â”œâ”€â”€ database.py        # Database connection settings
â”‚   â”‚   â”œâ”€â”€ ingestion.py       # Mediastack API & ingestion config
â”‚   â”‚   â”œâ”€â”€ crawler.py         # Web crawling settings
â”‚   â”‚   â”œâ”€â”€ scheduler.py       # Cloud Scheduler jobs & API optimization
â”‚   â”‚   â””â”€â”€ ui.py              # UI/frontend configuration
â”‚   â”œâ”€â”€ jobs/                  # Background processing
â”‚   â”‚   â”œâ”€â”€ run_ingestion.py   # Main ingestion pipeline
â”‚   â”‚   â”œâ”€â”€ run_crawl_worker.py # Crawl worker entry point
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ models/                # SQLAlchemy models
â”‚   â”œâ”€â”€ routers/               # FastAPI endpoints
â”‚   â”œâ”€â”€ schemas/               # Pydantic schemas
â”‚   â””â”€â”€ utils/                 # Utilities (sentiment, cleanup, secrets, robots.txt)
â”œâ”€â”€ tests/                     # Test suite
â”‚   â””â”€â”€ test_crawl_worker.py   # Worker tests
â”œâ”€â”€ scripts/                   # Development utilities
â”œâ”€â”€ docs/                      # Documentation
â”œâ”€â”€ alembic/                   # Database migrations
â”œâ”€â”€ docker-compose.yml         # Development multi-container
â”œâ”€â”€ docker-compose.prod.yml    # Production configuration
â””â”€â”€ .env                       # Environment variables (not committed)
```

### Service Architecture

- **ğŸŒ Web Service** (`docker/Dockerfile.web`): FastAPI API server with health checks
- **ğŸ•·ï¸ Worker Service** (`docker/Dockerfile.worker`): Background web crawling and content processing
- **â° Scheduler Service** (`docker/Dockerfile.scheduler`): Periodic ingestion from Mediastack API
- **ğŸ—„ï¸ Database Service**: PostgreSQL with automated migrations

## ğŸ§ª Development

### Code Quality
```bash
# Install pre-commit hooks
pre-commit install

# Run all quality checks
pre-commit run --all-files

# Individual tools
black .                    # Code formatting
isort .                   # Import sorting
flake8 .                  # Style checking
mypy .                    # Type checking
```

### Testing
```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=app

# Test specific module
pytest tests/test_ingestion.py -v
```

### Development Utilities
```bash
# Check recent articles
python scripts/check_articles.py

# Discover available sources
python scripts/discover_sources.py

# Clean up expired content and maintain database health
curl -X POST http://localhost:8000/api/v1/cleanup

# Run crawl worker (local)
python app/jobs/run_crawl_worker.py

# Monitor multi-container services
docker-compose logs -f web worker scheduler

# Service health checks
curl http://localhost:8080/health     # Web API health
curl http://localhost:8080/ready      # Kubernetes readiness
```

## ğŸ”§ Configuration

### Organized Configuration Structure

Configuration is organized into separate classes for better maintainability:

```python
# New organized approach (recommended)
from app.config import config

batch_size = config.ingestion.mediastack_fetch_limit
delay = config.crawler.crawler_default_delay
db_url = config.database.sqlalchemy_database_url

# Legacy approach (still works for backward compatibility)
from app.config import settings
batch_size = settings.MEDIASTACK_FETCH_LIMIT
```

**Configuration Classes:**
- `DatabaseConfig` - Database connections and environment settings
- `IngestionConfig` - Mediastack API and article ingestion settings
- `CrawlerConfig` - Web crawling and politeness configuration
- `SchedulerConfig` - Cloud Scheduler jobs and API usage optimization
- `UIConfig` - Frontend and display settings

### Environment Variables (.env)
```bash
# Database
LOCAL_DATABASE_URL=postgresql://user:pass@localhost:5432/aifeelnews_dev

# Mediastack API
MEDIASTACK_API_KEY=your_api_key_here
MEDIASTACK_BASE_URL=https://api.mediastack.com/v1/news
MEDIASTACK_FETCH_LIMIT=25

# Application
ENV=local
```

*Note: Environment variable names remain exactly the same. The new configuration structure provides better organization while maintaining full backward compatibility.*

### ğŸ” Security & Secret Management

**Production environments use Google Secret Manager for secure credential storage:**

- **API Keys**: `MEDIASTACK_API_KEY` â†’ `mediastack-api-key` secret
- **Database Passwords**: `POSTGRES_PASSWORD` â†’ `db-password` secret
- **Automatic Fallback**: Development environments use `.env` variables
- **Zero Code Changes**: Seamless transition between local and production

```python
# Configuration automatically handles both sources
from app.config import config
api_key = config.ingestion.mediastack_api_key  # Secret Manager or .env
```

**Security Features:**
- ğŸ”’ **Encrypted Storage**: Secrets encrypted at rest in Google Secret Manager
- ğŸ”„ **Environment Fallback**: Graceful fallback to environment variables
- ğŸš« **No Hardcoded Values**: All sensitive data externalized
- ğŸ”§ **Runtime Retrieval**: Secrets fetched securely at application startup

### Key Settings
- **Data Minimization**: Article bodies are never permanently stored (max 1024 chars)
- **TTL Cleanup**: Automatic cleanup of expired content snippets (7-day expiry)
- **Database Maintenance**: Automated cleanup of old crawl jobs and maintenance statistics
- **Rate Limiting**: Respectful crawling with domain-based delays and backoff
- **Robots.txt Compliance**: Full respect for website crawling policies
- **Ethical Crawling**: Honest User-Agent identification and request throttling

## ğŸ—ï¸ Multi-Service Architecture

### Container Services

**ğŸŒ Web API Service** (`docker/Dockerfile.web`)
- FastAPI application with OpenAPI docs
- Health check endpoints (`/health`, `/ready`)
- Database migrations on startup
- Optimized for GCP Cloud Run

**ğŸ•·ï¸ Background Worker Service** (`docker/Dockerfile.worker`)
- Processes crawl jobs from database queue
- Ethical web crawling with robots.txt compliance
- Content extraction and sentiment analysis
- Automatic retry with exponential backoff

**â° Scheduler Service** (`docker/Dockerfile.scheduler`)
- Periodic ingestion from Mediastack API
- Runs every hour in production
- Creates crawl jobs for worker processing
- Handles API rate limiting

### Data Flow
1. **ğŸ“¥ Scheduler**: Fetches metadata from Mediastack API â†’ creates crawl jobs
2. **ğŸ•·ï¸ Worker**: Processes crawl jobs â†’ extracts content â†’ runs sentiment analysis
3. **ğŸ’¾ Storage**: Persists minimal metadata with TTL content snippets
4. **ğŸŒ API**: Serves processed data via REST endpoints
5. **ğŸ§¹ Cleanup**: Automatic TTL cleanup of expired content

### Database Design
- `sources` - News source configuration
- `articles` - Article metadata with sentiment scores
- `crawl_jobs` - Web crawling queue with robots.txt compliance status
- `article_contents` - Short-term content snippets (TTL-managed, max 1024 chars)
- `sentiment_analyses` - Multiple provider sentiment results (VADER â†’ GCP NL)
- `users` & `bookmarks` - User functionality

### Ethical Crawling Framework
- **ğŸ¤– Robots.txt Compliance**: Full respect for website crawling policies
- **â±ï¸ Rate Limiting**: Domain-based delays with exponential backoff
- **ğŸ” Content Extraction**: BeautifulSoup-based text extraction (no full storage)
- **ğŸ“Š Status Tracking**: Comprehensive crawl job monitoring and error handling
- **ğŸ”’ Data Minimization**: Content truncated to 1024 chars with 7-day TTL

### Cloud-Ready Design
- **Container Isolation**: Separate services for independent scaling
- **Health Monitoring**: Kubernetes-compatible health and readiness probes
- **Secret Management**: Google Secret Manager for production credential security
- **Environment Configuration**: 12-factor app methodology with secure secret handling
- **Stateless Services**: Database-driven job queuing for horizontal scaling

**Privacy & Ethics**: Full article bodies are never stored to respect copyright and minimize data footprint. Only metadata and brief excerpts with automatic expiration.

### â° Cloud Scheduler Optimization

**Automated News Ingestion** with API usage optimization:

- **Schedule**: Every 8 hours (3 times daily: 00:00, 08:00, 16:00 UTC)
- **API Usage**: 45.7% of 10,000 monthly request limit
- **Daily Output**: ~3,750 articles with optimal freshness
- **Safety Buffer**: 54.3% remaining for traffic spikes and development

**Automated Database Cleanup**:

- **Schedule**: Daily at 2:00 AM UTC
- **TTL Cleanup**: Removes expired article content (respects privacy requirements)
- **Maintenance**: Cleans up old crawl jobs (7+ days old completed/failed records)
- **Statistics**: Provides comprehensive database health metrics

**Key Features:**
- **ğŸ¯ API Efficiency**: 50 requests per run Ã— 3 daily = 4,566 monthly requests
- **ğŸ”„ Automatic Scaling**: Cloud Run handles traffic bursts seamlessly
- **ğŸ§¹ Database Health**: Automated cleanup prevents database bloat
- **ğŸ“Š Usage Monitoring**: Built-in estimation and tracking
- **âš™ï¸ Configuration**: Easily adjustable via `SchedulerConfig`

**Setup Commands:**
```bash
# Deploy updated container with scheduler integration
docker build -f docker/Dockerfile.web -t gcr.io/project/aifeelnews-web .
docker push gcr.io/project/aifeelnews-web
gcloud run deploy aifeelnews-web --image gcr.io/project/aifeelnews-web

# Create Cloud Scheduler jobs (after deployment)
python scripts/create-scheduler-commands.py
# Or use the interactive setup script:
# python scripts/setup-cloud-scheduler.py

# Verify jobs are created
gcloud scheduler jobs list --location=europe-west1

# Test jobs manually
gcloud scheduler jobs run aifeelnews-ingestion --location=europe-west1
gcloud scheduler jobs run aifeelnews-cleanup --location=europe-west1
```

## ğŸ“ˆ Monitoring & Maintenance

### Multi-Container Monitoring
```bash
# Check all service health
docker-compose ps

# View real-time logs from all services
docker-compose logs -f

# Monitor specific services
docker-compose logs -f web worker scheduler

# Check service health endpoints
curl http://localhost:8080/health     # API health with DB connectivity
curl http://localhost:8080/ready      # Readiness probe

# View API documentation
curl http://localhost:8080/docs
```

### Database & Jobs
```bash
# Check recent articles and crawl status
python scripts/check_articles.py

# Run one-time ingestion
docker-compose exec scheduler python -m app.jobs.run_ingestion

# Database cleanup and maintenance (automated via Cloud Scheduler)
curl -X POST http://localhost:8000/api/v1/cleanup

# Check worker job processing
docker-compose exec worker python app/jobs/run_crawl_worker.py --dry-run
```

### Production Monitoring
- **Health Checks**: `/health` endpoint tests database connectivity
- **Readiness Probes**: `/ready` endpoint for Kubernetes deployment
- **Service Logs**: Structured logging with correlation IDs
- **Job Monitoring**: Database-driven crawl job status tracking

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Install pre-commit hooks (`pre-commit install`)
4. Make changes with proper tests
5. Ensure all quality checks pass (`pre-commit run --all-files`)
6. Commit changes (`git commit -m 'Add amazing feature'`)
7. Push to branch (`git push origin feature/amazing-feature`)
8. Open Pull Request

## ğŸ“„ License

This project is for educational/research/assessment purposes.

## ğŸ†˜ Troubleshooting

### Common Issues

**Multi-Container Issues**
```bash
# Rebuild containers after code changes
docker-compose build

# Check service logs for errors
docker-compose logs web worker scheduler

# Restart specific services
docker-compose restart worker

# Clean restart all services
docker-compose down && docker-compose up -d
```

**Database Connection Error**
```bash
# Check PostgreSQL container is running
docker-compose ps db

# Test database connectivity
docker-compose exec db psql -U postgres -d aifeelnews -c "SELECT 1;"

# Check environment variables are loaded
docker-compose exec web env | grep DATABASE_URL
```

**Health Check Failures**
```bash
# Test health endpoints directly
curl -v http://localhost:8080/health
curl -v http://localhost:8080/ready

# Check web service logs
docker-compose logs web

# Verify database migrations
docker-compose exec web alembic current
```

**Mediastack API Issues**
```bash
# Verify API key in container
docker-compose exec scheduler env | grep MEDIASTACK

# Test API directly
curl "https://api.mediastack.com/v1/news?access_key=YOUR_KEY&limit=1"

# Run ingestion with debugging
docker-compose exec scheduler python -m app.jobs.run_ingestion
```

For more help, check the logs or open an issue!
