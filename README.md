# aiFeelNews ğŸ“°ğŸ’­

> **AI-powered news sentiment analysis platform for university project with research purposes**

A FastAPI-based news aggregation and sentiment analysis platform that ingests articles from Mediastack API, processes them with sentiment analysis, and provides a RESTful API for accessing news with emotional context.

## ğŸ¯ Project Overview

**Core Features:**
- ğŸ“° **News Ingestion**: Automated Mediastack API integration with English-only filtering
- ğŸ•·ï¸ **Ethical Web Crawling**: Full robots.txt compliance with respectful rate limiting
- ğŸ§  **Advanced Sentiment Analysis**: Multi-provider architecture (VADER + Google Cloud NL)
- ğŸ—„ï¸ **Privacy-First Database**: PostgreSQL with TTL-based content expiry and data minimization
- ğŸš€ **Production API**: FastAPI with OpenAPI documentation and health endpoints
- â° **Automated Content Lifecycle**: TTL-based cleanup preventing long-term content storage
- ğŸ›¡ï¸ **Security & Compliance**: Honest User-Agent, domain-based rate limiting, error handling
- ğŸ“… **CI/CD Pipeline**: GitHub Actions with automated testing, building, and Cloud Run deployment
- ğŸ›ï¸ **Configuration Management**: Environment-based settings with provider switching capabilities

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- PostgreSQL (or use Docker Compose)
- Mediastack API key

## ğŸ¯ CI/CD Pipeline

**GitHub Actions Automated Deployment** - Full CI/CD pipeline with Google Cloud Platform:

### Pipeline Features
- **âœ… Automated Testing**: Runs flake8, mypy, and pytest on every commit
- **ğŸ³ Container Build**: Builds and pushes Docker images to Google Container Registry
- **â˜ï¸ Cloud Deployment**: Deploys to Google Cloud Run with zero-downtime updates
- **ğŸ” Secure Authentication**: Service account-based authentication with proper IAM roles
- **ğŸ“Š Quality Gates**: Code must pass all quality checks before deployment

### Pipeline Stages
1. **Test Stage**: Code quality checks (flake8, mypy, pytest with SQLite in-memory database)
2. **Build Stage**: Docker image build and push to `gcr.io/aifeelnews-prod/aifeelnews-web`
3. **Deploy Stage**: Cloud Run service deployment with environment variable injection

### Automated Deployment
Every push to `main` branch triggers:
```yaml
# .github/workflows/deploy.yml
name: Deploy to Cloud Run
on:
  push:
    branches: [ main ]

jobs:
  test:
    - Code quality checks (flake8, mypy)
    - Unit tests with SQLite test database

  build-and-deploy:
    - Docker build and push to GCR
    - Cloud Run deployment
    - Health check verification
```

**Production URL**: https://aifeelnews-web-813770885946.europe-west1.run.app

### CI/CD Service Account Setup
```bash
# Service account with required roles
gcloud iam service-accounts create github-actions-sa --display-name="GitHub Actions"

# Grant necessary permissions
gcloud projects add-iam-policy-binding aifeelnews-prod \
  --member="serviceAccount:github-actions-sa@aifeelnews-prod.iam.gserviceaccount.com" \
  --role="roles/run.admin"

gcloud projects add-iam-policy-binding aifeelnews-prod \
  --member="serviceAccount:github-actions-sa@aifeelnews-prod.iam.gserviceaccount.com" \
  --role="roles/storage.admin"

gcloud projects add-iam-policy-binding aifeelnews-prod \
  --member="serviceAccount:github-actions-sa@aifeelnews-prod.iam.gserviceaccount.com" \
  --role="roles/artifactregistry.writer"

# Generate and store key as GitHub secret
gcloud iam service-accounts keys create github-actions-key.json \
  --iam-account=github-actions-sa@aifeelnews-prod.iam.gserviceaccount.com
```

**GitHub Secrets Required**:
- `GCP_SA_KEY`: Service account JSON key for authentication
- `GCP_PROJECT_ID`: Google Cloud project ID (`aifeelnews-prod`)

### Manual Deployment (if needed)
```bash
# Build and deploy manually
docker build -f docker/Dockerfile.web -t gcr.io/aifeelnews-prod/aifeelnews-web:latest .
docker push gcr.io/aifeelnews-prod/aifeelnews-web:latest

gcloud run deploy aifeelnews-web \
  --image gcr.io/aifeelnews-prod/aifeelnews-web:latest \
  --region europe-west1 \
  --allow-unauthenticated \
  --set-env-vars ENV=production
```

### Local Development

```bash
# Clone and setup
git clone <repository-url>
cd aifeelnews

# Install dependencies
pip install -r requirements.txt

# Environment setup
cp .env.example .env
# Edit .env with your Mediastack API key and database URL

# Database setup
alembic upgrade head

# Run ingestion pipeline
python -m app.jobs.run_ingestion

# Run crawl worker separately
python app/jobs/run_crawl_worker.py

# Start API server
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Multi-Container Docker

```bash
# Start all services (web API, worker, scheduler, database)
docker-compose up -d

# Check service health
docker-compose ps
curl http://localhost:8080/health

# View logs for specific services
docker-compose logs web
docker-compose logs worker
docker-compose logs scheduler

# Run one-time ingestion
docker-compose exec scheduler python -m app.jobs.run_ingestion

# Stop all services
docker-compose down
```

### Production Docker

```bash
# Test production configuration locally
docker-compose -f docker-compose.prod.yml build
docker-compose -f docker-compose.prod.yml up

# Build individual services
docker build -f docker/Dockerfile.web -t aifeelnews-web .
docker build -f docker/Dockerfile.worker -t aifeelnews-worker .
docker build -f docker/Dockerfile.scheduler -t aifeelnews-scheduler .
```

## ğŸ“Š API Endpoints

| Endpoint | Method | Description |
|----------|---------|-------------|
| `/docs` | GET | Interactive API documentation |
| `/health` | GET | Service health check with database connectivity |
| `/ready` | GET | Kubernetes readiness check |
| `/articles` | GET | List articles with sentiment data |
| `/articles/{id}` | GET | Get specific article details |
| `/sources` | GET | List configured news sources |
| `/users` | GET/POST | User management |
| `/bookmarks` | GET/POST | User bookmarks |
| `/api/v1/trigger-ingestion` | POST | **Cloud Scheduler**: Trigger news ingestion pipeline |
| `/api/v1/cleanup` | POST | **Cloud Scheduler**: Database maintenance and TTL cleanup |

**Example API Response:**
```json
{
  "id": 1,
  "title": "Breaking: AI Revolution in Healthcare",
  "description": "New developments in medical AI...",
  "url": "https://example.com/article",
  "published_at": "2024-11-18T15:30:00Z",
  "sentiment_label": "positive",
  "sentiment_score": 0.8472,
  "source": {
    "name": "TechNews",
    "domain": "technews.com"
  }
}
```

## ğŸ—‚ï¸ Project Structure

**Multi-Container Architecture:** Separate services for scalability and deployment flexibility.

```
aifeelnews/
â”œâ”€â”€ docker/                     # Container configurations
â”‚   â”œâ”€â”€ Dockerfile.web          # FastAPI web service
â”‚   â”œâ”€â”€ Dockerfile.worker       # Background crawl worker
â”‚   â”œâ”€â”€ Dockerfile.scheduler    # Scheduled ingestion jobs
â”‚   â””â”€â”€ README.md              # Docker documentation
â”œâ”€â”€ app/                       # Application code
â”‚   â”œâ”€â”€ config/                # Organized configuration classes
â”‚   â”‚   â”œâ”€â”€ __init__.py        # Main config object & legacy compatibility
â”‚   â”‚   â”œâ”€â”€ database.py        # Database connection settings
â”‚   â”‚   â”œâ”€â”€ ingestion.py       # Mediastack API & ingestion config
â”‚   â”‚   â”œâ”€â”€ crawler.py         # Web crawling settings
â”‚   â”‚   â”œâ”€â”€ scheduler.py       # Cloud Scheduler jobs & API optimization
â”‚   â”‚   â””â”€â”€ ui.py              # UI/frontend configuration
â”‚   â”œâ”€â”€ jobs/                  # Background processing
â”‚   â”‚   â”œâ”€â”€ run_ingestion.py   # Main ingestion pipeline
â”‚   â”‚   â”œâ”€â”€ run_crawl_worker.py # Crawl worker entry point
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ models/                # SQLAlchemy models
â”‚   â”œâ”€â”€ routers/               # FastAPI endpoints
â”‚   â”œâ”€â”€ schemas/               # Pydantic schemas
â”‚   â””â”€â”€ utils/                 # Utilities (sentiment, cleanup, secrets, robots.txt)
â”œâ”€â”€ tests/                     # Test suite
â”‚   â””â”€â”€ test_crawl_worker.py   # Worker tests
â”œâ”€â”€ scripts/                   # Development utilities
â”œâ”€â”€ docs/                      # Documentation
â”œâ”€â”€ alembic/                   # Database migrations
â”œâ”€â”€ docker-compose.yml         # Development multi-container
â”œâ”€â”€ docker-compose.prod.yml    # Production configuration
â””â”€â”€ .env                       # Environment variables (not committed)
```

### Service Architecture

- **ğŸŒ Web Service** (`docker/Dockerfile.web`): FastAPI API server with health checks
- **ğŸ•·ï¸ Worker Service** (`docker/Dockerfile.worker`): Background web crawling and content processing
- **â° Scheduler Service** (`docker/Dockerfile.scheduler`): Periodic ingestion from Mediastack API
- **ğŸ—„ï¸ Database Service**: PostgreSQL with automated migrations

## ğŸ§ª Development

### Code Quality
```bash
# Install pre-commit hooks
pre-commit install

# Run all quality checks
pre-commit run --all-files

# Individual tools
black .                    # Code formatting
isort .                   # Import sorting
flake8 .                  # Style checking
mypy .                    # Type checking
```

### Testing
```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=app

# Test specific module
pytest tests/test_ingestion.py -v
```

### Development Utilities
```bash
# Check recent articles
python scripts/check_articles.py

# Discover available sources
python scripts/discover_sources.py

# Clean up expired content and maintain database health
curl -X POST http://localhost:8000/api/v1/cleanup

# Run crawl worker (local)
python app/jobs/run_crawl_worker.py

# Monitor multi-container services
docker-compose logs -f web worker scheduler

# Service health checks
curl http://localhost:8080/health     # Web API health
curl http://localhost:8080/ready      # Kubernetes readiness
```

## ğŸ”§ Configuration

### Organized Configuration Structure

Configuration is organized into separate classes for better maintainability:

```python
# New organized approach (recommended)
from app.config import config

batch_size = config.ingestion.mediastack_fetch_limit
delay = config.crawler.crawler_default_delay
db_url = config.database.sqlalchemy_database_url

# Legacy approach (still works for backward compatibility)
from app.config import settings
batch_size = settings.MEDIASTACK_FETCH_LIMIT
```

**Configuration Classes:**
- `DatabaseConfig` - Database connections and environment settings
- `IngestionConfig` - Mediastack API and article ingestion settings
- `CrawlerConfig` - Web crawling and politeness configuration
- `SchedulerConfig` - Cloud Scheduler jobs and API usage optimization
- `UIConfig` - Frontend and display settings

### Environment Variables (.env)
```bash
# Database
LOCAL_DATABASE_URL=postgresql://user:pass@localhost:5432/aifeelnews_dev

# Mediastack API
MEDIASTACK_API_KEY=your_api_key_here
MEDIASTACK_BASE_URL=https://api.mediastack.com/v1/news
MEDIASTACK_FETCH_LIMIT=25

# Application
ENV=local
```

*Note: Environment variable names remain exactly the same. The new configuration structure provides better organization while maintaining full backward compatibility.*

### ğŸ” Security & Secret Management

**Production environments use Google Secret Manager for secure credential storage:**

- **API Keys**: `MEDIASTACK_API_KEY` â†’ `mediastack-api-key` secret
- **Database Passwords**: `POSTGRES_PASSWORD` â†’ `db-password` secret
- **Automatic Fallback**: Development environments use `.env` variables
- **Zero Code Changes**: Seamless transition between local and production

```python
# Configuration automatically handles both sources
from app.config import config
api_key = config.ingestion.mediastack_api_key  # Secret Manager or .env
```

**Security Features:**
- ğŸ”’ **Encrypted Storage**: Secrets encrypted at rest in Google Secret Manager
- ğŸ”„ **Environment Fallback**: Graceful fallback to environment variables
- ğŸš« **No Hardcoded Values**: All sensitive data externalized
- ğŸ”§ **Runtime Retrieval**: Secrets fetched securely at application startup

### Key Settings
- **Data Minimization**: Article bodies are never permanently stored (max 1024 chars)
- **TTL Cleanup**: Automatic cleanup of expired content snippets (7-day expiry)
- **Database Maintenance**: Automated cleanup of old crawl jobs and maintenance statistics
- **Rate Limiting**: Respectful crawling with domain-based delays and backoff
- **Robots.txt Compliance**: Full respect for website crawling policies
- **Ethical Crawling**: Honest User-Agent identification and request throttling

## ğŸ§  Advanced Sentiment Analysis

### Multi-Provider Architecture
**Flexible Provider Selection**: Environment-configurable sentiment analysis with seamless switching:

**ğŸ†“ VADER (Default)**:
- Fast, offline lexicon-based analysis
- Perfect for development and testing
- Zero API costs and dependencies
- Optimized for social media and news text

**â˜ï¸ Google Cloud Natural Language (Production)**:
- Enterprise-grade ML sentiment analysis
- Production-quality accuracy and insights
- Magnitude scoring for emotional intensity
- Automatic fallback to VADER on errors

### Configuration
```bash
# Environment variable switching
SENTIMENT_PROVIDER=VADER          # Free, fast (default)
SENTIMENT_PROVIDER=GCP_NL         # Production ML analysis

# Google Cloud setup (optional)
GOOGLE_APPLICATION_CREDENTIALS=path/to/credentials.json
```

### Optimization Features
- **English-Only Processing**: Optimized for our English news pipeline (no language detection overhead)
- **Graceful Fallbacks**: Automatic VADER fallback when GCP NL is unavailable
- **Error Handling**: Robust error handling prevents sentiment analysis failures from breaking ingestion
- **Provider Abstraction**: Consistent API regardless of underlying provider

## ğŸ—ï¸ Multi-Service Architecture

### Container Services

**ğŸŒ Web API Service** (`docker/Dockerfile.web`)
- FastAPI application with OpenAPI docs
- Health check endpoints (`/health`, `/ready`)
- Database migrations on startup
- Optimized for GCP Cloud Run

**ğŸ•·ï¸ Background Worker Service** (`docker/Dockerfile.worker`)
- Processes crawl jobs from database queue
- Ethical web crawling with robots.txt compliance
- Content extraction and sentiment analysis
- Automatic retry with exponential backoff

**â° Scheduler Service** (`docker/Dockerfile.scheduler`)
- Periodic ingestion from Mediastack API
- Runs every hour in production
- Creates crawl jobs for worker processing
- Handles API rate limiting

### Data Flow
1. **ğŸ“¥ Scheduler**: Fetches metadata from Mediastack API â†’ creates crawl jobs
2. **ğŸ•·ï¸ Worker**: Processes crawl jobs â†’ extracts content â†’ runs sentiment analysis
3. **ğŸ’¾ Storage**: Persists minimal metadata with TTL content snippets
4. **ğŸŒ API**: Serves processed data via REST endpoints
5. **ğŸ§¹ Cleanup**: Automatic TTL cleanup of expired content

### Database Design
- `sources` - News source configuration
- `articles` - Article metadata with sentiment scores
- `crawl_jobs` - Web crawling queue with robots.txt compliance status
- `article_contents` - Short-term content snippets (TTL-managed, max 1024 chars)
- `sentiment_analyses` - Multiple provider sentiment results (VADER â†’ GCP NL)
- `users` & `bookmarks` - User functionality

### Ethical Crawling Framework
- **ğŸ¤– Robots.txt Compliance**: Full respect for website crawling policies
- **â±ï¸ Rate Limiting**: Domain-based delays with exponential backoff
- **ğŸ” Content Extraction**: BeautifulSoup-based text extraction (no full storage)
- **ğŸ“Š Status Tracking**: Comprehensive crawl job monitoring and error handling
- **ğŸ”’ Data Minimization**: Content truncated to 1024 chars with 7-day TTL

### Cloud-Ready Design
- **Container Isolation**: Separate services for independent scaling
- **Health Monitoring**: Kubernetes-compatible health and readiness probes
- **Secret Management**: Google Secret Manager for production credential security
- **Environment Configuration**: 12-factor app methodology with secure secret handling
- **Stateless Services**: Database-driven job queuing for horizontal scaling

**Privacy & Ethics**: Full article bodies are never stored to respect copyright and minimize data footprint. Only metadata and brief excerpts with automatic expiration.

### â° Cloud Scheduler Optimization

**Automated News Ingestion** with API usage optimization:

- **Schedule**: Every 8 hours (3 times daily: 00:00, 08:00, 16:00 UTC)
- **API Usage**: 45.7% of 10,000 monthly request limit
- **Daily Output**: ~3,750 articles with optimal freshness
- **Safety Buffer**: 54.3% remaining for traffic spikes and development

**Automated Database Cleanup**:

- **Schedule**: Daily at 2:00 AM UTC
- **TTL Cleanup**: Removes expired article content (respects privacy requirements)
- **Maintenance**: Cleans up old crawl jobs (7+ days old completed/failed records)
- **Statistics**: Provides comprehensive database health metrics

**Key Features:**
- **ğŸ¯ API Efficiency**: 50 requests per run Ã— 3 daily = 4,566 monthly requests
- **ğŸ”„ Automatic Scaling**: Cloud Run handles traffic bursts seamlessly
- **ğŸ§¹ Database Health**: Automated cleanup prevents database bloat
- **ğŸ“Š Usage Monitoring**: Built-in estimation and tracking
- **âš™ï¸ Configuration**: Easily adjustable via `SchedulerConfig`

**Setup Commands:**
```bash
# Deploy updated container with scheduler integration
docker build -f docker/Dockerfile.web -t gcr.io/project/aifeelnews-web .
docker push gcr.io/project/aifeelnews-web
gcloud run deploy aifeelnews-web --image gcr.io/project/aifeelnews-web

# Create Cloud Scheduler jobs (after deployment)
python scripts/create-scheduler-commands.py
# Or use the interactive setup script:
# python scripts/setup-cloud-scheduler.py

# Verify jobs are created
gcloud scheduler jobs list --location=europe-west1

# Test jobs manually
gcloud scheduler jobs run aifeelnews-ingestion --location=europe-west1
gcloud scheduler jobs run aifeelnews-cleanup --location=europe-west1
```

## ğŸ“ˆ Monitoring & Maintenance

### Multi-Container Monitoring
```bash
# Check all service health
docker-compose ps

# View real-time logs from all services
docker-compose logs -f

# Monitor specific services
docker-compose logs -f web worker scheduler

# Check service health endpoints
curl http://localhost:8080/health     # API health with DB connectivity
curl http://localhost:8080/ready      # Readiness probe

# View API documentation
curl http://localhost:8080/docs
```

### Database & Jobs
```bash
# Check recent articles and crawl status
python scripts/check_articles.py

# Run one-time ingestion
docker-compose exec scheduler python -m app.jobs.run_ingestion

# Database cleanup and maintenance (automated via Cloud Scheduler)
curl -X POST http://localhost:8000/api/v1/cleanup

# Check worker job processing
docker-compose exec worker python app/jobs/run_crawl_worker.py --dry-run
```

### Production Monitoring
- **Health Checks**: `/health` endpoint tests database connectivity
- **Readiness Probes**: `/ready` endpoint for Kubernetes deployment
- **Service Logs**: Structured logging with correlation IDs
- **Job Monitoring**: Database-driven crawl job status tracking

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Install pre-commit hooks (`pre-commit install`)
4. Make changes with proper tests
5. Ensure all quality checks pass (`pre-commit run --all-files`)
6. Commit changes (`git commit -m 'Add amazing feature'`)
7. Push to branch (`git push origin feature/amazing-feature`)
8. Open Pull Request

## ğŸ“„ License

This project is for educational/research/assessment purposes.

## ğŸ†˜ Troubleshooting

### Common Issues

**Multi-Container Issues**
```bash
# Rebuild containers after code changes
docker-compose build

# Check service logs for errors
docker-compose logs web worker scheduler

# Restart specific services
docker-compose restart worker

# Clean restart all services
docker-compose down && docker-compose up -d
```

**Database Connection Error**
```bash
# Check PostgreSQL container is running
docker-compose ps db

# Test database connectivity
docker-compose exec db psql -U postgres -d aifeelnews -c "SELECT 1;"

# Check environment variables are loaded
docker-compose exec web env | grep DATABASE_URL
```

**Health Check Failures**
```bash
# Test health endpoints directly
curl -v http://localhost:8080/health
curl -v http://localhost:8080/ready

# Check web service logs
docker-compose logs web

# Verify database migrations
docker-compose exec web alembic current
```

**Mediastack API Issues**
```bash
# Verify API key in container
docker-compose exec scheduler env | grep MEDIASTACK

# Test API directly
curl "https://api.mediastack.com/v1/news?access_key=YOUR_KEY&limit=1"

# Run ingestion with debugging
docker-compose exec scheduler python -m app.jobs.run_ingestion
```

For more help, check the logs or open an issue!
